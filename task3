import pandas as pd
# import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
# import statsmodels.formula.api as sm
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from mlxtend.feature_selection import SequentialFeatureSelector as sfs
from sklearn.model_selection import KFold
# from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler

#backword elimination + k-fold + standartization
cv = KFold(n_splits=2, random_state=0, shuffle=False)
def backwordelem(regressor,X,y,dataset):
    
    sfs1=sfs(regressor, k_features=8, forward=False, verbose=1,cv=cv, scoring='neg_mean_squared_error')
    sfs1=sfs1.fit(dataset.iloc[:,5:23], y)
    list_names= list(sfs1.k_feature_names_)
    dataset_new=dataset[list_names]
    X=dataset_new.iloc[:].values
    X_train, X_test, y_train, y_test=train_test_split(X, y, test_size = 0.2, random_state = 0)
    #scaling the data
    sc_x = StandardScaler()
    X_train = sc_x.fit_transform(X_train)
    X_test = sc_x.transform(X_test) 
    return X_train,X_test,y_train, y_test
 #
 
dataset = pd.read_csv('COVID-19.csv')
index = dataset.index
n = len(index)
 
#correlation plot   
fig_dims = (12, 8)
fig, ax = plt.subplots(figsize=fig_dims)
sns.heatmap(dataset.iloc[:,2:23].corr(), ax=ax)
plt.show()

#columns variation
print(abs(dataset.iloc[:,2:23].corr()['Positive Tests']))

#data after taking out 3 columns with no corelation to the depended variabol
X = dataset.iloc[:,[6,7,8,9,10,11,12,14,15,16,17,18,19,20,22]].values

#check for multicolenarity
#vif = [variance_inflation_factor(X,i) for i in range(dataset.iloc[:,5:23].shape[1])]
#print(vif)


for i in [3,4]: # start with positive column and then the negative column
    sns.set_theme()
    compering_table=[]
    
    # בשלב הראשוני נפתר מהפיצרים עם השונות 
    y = dataset.iloc[:, i].values

    # Splitting the dataset into the Training set and Test se
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
    
    ########################## linear regression #######################
    
    regressor = LinearRegression()
    X_train, X_test, y_train, y_test=backwordelem(regressor,X,y,dataset)
    regressor.fit(X_train, y_train)

    # Predicting the Test set results
    y_pred = regressor.predict(X_test)
    error = (y_pred-y_test)#*(y_pred-y_test)
    r2_linear=r2_score(y_test , y_pred)
    #error = sum (error)
    b_0=regressor.intercept_
    b_1=regressor.coef_
    compering_table.append(["linear",r2_linear])
    g=sns.regplot(y_test , y_pred,scatter_kws = {'color': 'blue'}, line_kws = {'color': 'red'}).set_title('linear regression')
    #g.set(ylim=(0, 25000),xlim=(0, 25000))
    plt.show()
    scores = []
    scores.append({
            'model': "linear regression",
            'best_score': "_",
            'best_params': " b0 = "+str(b_0)+" b1 = "+ str(b_1 )
        })
    ## בשלב זה צריך לעשות אילמנציה כמו שלמדנו בכיתה
    
    #p######################### polynom regression#####################
    from sklearn.preprocessing import PolynomialFeatures
    
    for j in range(1,6):
        degree=j# מספר דרגות החופש במשוואה
        polyreg=PolynomialFeatures()
        
        x_train_poly , x_test_poly = polyreg.fit_transform(X_train) , polyreg.fit_transform(X_test)
        model=LinearRegression()
        model.fit(x_train_poly,y_train)
        y_pred = model.predict( x_test_poly)
        r2=r2_score(y_test , y_pred)
        adj_r2=1-(1-r2)*((n-1)/n-19-1)
        print(adj_r2)
    # לפי החישובים ראינו שלא משנה דרגת החופש שבוחרים, השגיאה נשארת זהה ושואפת ל1 לכן נבחר דרגת חופש 2
    
    degree=2
    polyreg=PolynomialFeatures(degree=2)

    x_train_poly , x_test_poly = polyreg.fit_transform(X_train) , polyreg.fit_transform(X_test)
    
    model=LinearRegression()
    model.fit(x_train_poly,y_train)
    y_pred_poly = model.predict(x_test_poly)
    r2_poly=r2_score(y_test , y_pred_poly)
    b_0_poly=model.intercept_
    b_1_poly=model.coef_
    compering_table.append(["poly",r2_poly])
    sns.regplot(y_test, y_pred_poly,scatter_kws = {'color': 'green'}, line_kws = {'color': 'red'}).set_title('polinomial regression')

    plt.show()
    
    scores.append({
            'model': "polynomial",
            'best_score': "_",
            'best_params': "2 degrees b0 = "+str(b_0_poly)+" b's =  "+str(b_1_poly )
        })
    
    ################################### cross validation ##############################
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import Ridge
    from sklearn.model_selection import GridSearchCV
    from numpy import arange
    from sklearn.linear_model import Lasso
    from sklearn.neighbors import KNeighborsClassifier
    
    model_params = {
        'ridge': {
            'model': Ridge(),
            'params' : {
                'alpha': arange(0, 5, 0.1) ,
            }  
        },
        'lasso': {
            'model': Lasso(),
            'params' : {
                'alpha': arange(0,5, 0.1) 
            }  
        },
        'random_forest': {
            'model': RandomForestClassifier(),
            'params' : {
                'n_estimators': [1,2,3,4,5,10],
                'max_depth':[2,5,7,10,15,50,100]
            }
        },
        'KNeighbors_Classifier': {
            'model': KNeighborsClassifier(),
            'params' : {
                'n_neighbors': [1,2,3,4,5,10,20]
                
            }
        },
    }
    
    
    
    for model_name, mp in model_params.items():
        clf =  GridSearchCV(mp['model'], mp['params'], cv=2, return_train_score=False)
        clf.fit(X, y)
        scores.append({
            'model': model_name,
            'best_score': clf.best_score_,
            'best_params': clf.best_params_
        })
        
    
    df = pd.DataFrame(scores,columns=['model','best_score','best_params'])
    
    ############################### ridge regression ##############################
    
    fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
    
    model_ridge=Ridge(alpha=0.1)
    
    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(model_ridge,X,y,dataset)
        
    model_ridge.fit(X_train,y_train)
    y_pred_ridge = model_ridge.predict(X_test)
    r2_ridge=r2_score(y_test , y_pred_ridge)
    
    sns.regplot(y_test,  y_pred_ridge,scatter_kws = {'color': 'brown'}, line_kws = {'color': 'red'},ax=ax1).set_title('alpha = 0.1 : r2 = '+str(round(r2_ridge,2)))
    
    model_ridge=Ridge(alpha=5)

    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(model_ridge,X,y,dataset)
    
    model_ridge.fit(X_train,y_train)
    y_pred_ridge = model_ridge.predict(X_test)
    r2_ridge=r2_score(y_test , y_pred_ridge)
    
    sns.regplot(y_test,  y_pred_ridge,scatter_kws = {'color': 'brown'}, line_kws = {'color': 'red'},ax=ax2).set_title(' alpha = 5 : r2 = '+str(round(r2_ridge,2)))
    
    model_ridge=Ridge(alpha=df.iloc[2]['best_params']['alpha'])

    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(model_ridge,X,y,dataset)


    model_ridge.fit(X_train,y_train)
    y_pred_ridge = model_ridge.predict(X_test)
    r2_ridge=r2_score(y_test , y_pred_ridge)
    b_0_ridge=model_ridge.intercept_
    b_1_ridge=model_ridge.coef_
    compering_table.append(["ridge",r2_ridge])
    
    sns.regplot(y_test,  y_pred_ridge,scatter_kws = {'color': 'brown'}, line_kws = {'color': 'red'},ax=ax3).set_title('best alpha = '+str(df.iloc[3]['best_params']['alpha'])+" r2 = "+str(round(r2_ridge,2)))
    
    fig.suptitle('compering different parameters in ridge regression')
     
    plt.show()
    
    plt.clf()
    
    ################################ lasso regression #############################
    
    fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
    
    model_lasso=Lasso(alpha=0.1)

    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(model_lasso,X,y,dataset)
    
    model_lasso.fit(X_train,y_train)
    y_pred_lasso = model_lasso.predict(X_test)
    r2_lasso=r2_score(y_test , y_pred_lasso)
    
    sns.regplot(y_test,  y_pred_lasso,scatter_kws = {'color': 'black'}, line_kws = {'color': 'red'},ax=ax1).set_title('alpha = 0.1 : r2 = '+str(round(r2_lasso,2)))
    
    model_lasso=Lasso(alpha=5)
    
    # backword elimination
    X_train, X_test, y_train, y_test=backwordelem(model_lasso,X,y,dataset)
    
    model_lasso.fit(X_train,y_train)
    y_pred_lasso = model_lasso.predict(X_test)
    r2_lasso=r2_score(y_test , y_pred_lasso)

    
    sns.regplot(y_test,  y_pred_lasso,scatter_kws = {'color': 'black'}, line_kws = {'color': 'red'},ax=ax2).set_title(' alpha =5 : r2 = '+str(round(r2_lasso,2)))

    model_lasso=Lasso(alpha=df.iloc[3]['best_params']['alpha'])
    
    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(model_lasso,X,y,dataset)
    
    model_lasso.fit(X_train,y_train)
    y_pred_lasso = model_lasso.predict(X_test)
    r2_lasso=r2_score(y_test , y_pred_lasso)
    b_0_lasso=model_lasso.intercept_
    b_1_lasso=model_lasso.coef_
    compering_table.append(["lasso",r2_lasso])
    sns.regplot(y_test,  y_pred_lasso,scatter_kws = {'color': 'black'}, line_kws = {'color': 'red'},ax=ax3).set_title('best alpha = '+str(df.iloc[3]['best_params']['alpha'])+" r2 = "+str(round(r2_lasso,2)))
    
    fig.suptitle('compering different parameters in lasso regression')
     
    plt.show()
    
    plt.clf()
    
    #################################### random forest ##############################
    
    fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(20, 5), sharey=True)
    
    # random forest n_estimators=1,max_depth=2
    RandomForest=RandomForestClassifier(n_estimators=1,max_depth=2)
    
    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(RandomForest,X,y,dataset)
    
    RandomForest.fit(X_train,y_train)
    y_pred_RandomForest = RandomForest.predict(X_test)
    r2_RandomForest=r2_score(y_test , y_pred_RandomForest)

    sns.regplot(y_test, y_pred_RandomForest,scatter_kws = {'color': 'purple'}, line_kws = {'color': 'red'},ax=ax1).set_title('minimum n=1 depth =2 : r2 = '+str(round(r2_RandomForest,2)))
    
    # random forest n_estimators=3,max_depth=7
    
    RandomForest=RandomForestClassifier(n_estimators=3,max_depth=7)
    
    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(RandomForest,X,y,dataset)
    
    RandomForest.fit(X_train,y_train)
    y_pred_RandomForest = RandomForest.predict(X_test)
    r2_RandomForest=r2_score(y_test , y_pred_RandomForest)
    
    sns.regplot(y_test, y_pred_RandomForest,scatter_kws = {'color': 'purple'}, line_kws = {'color': 'red'},ax=ax2).set_title('maximum n=3 depth =7 : r2 = '+str(round(r2_RandomForest,2)))
    
    #random forest with the best parameters

    RandomForest=RandomForestClassifier(n_estimators=df.iloc[4]['best_params']['n_estimators'],max_depth=df.iloc[4]['best_params']['max_depth'])
    
    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(RandomForest,X,y,dataset)
    
    RandomForest.fit(X_train,y_train)
    y_pred_RandomForest = RandomForest.predict(X_test)
    r2_RandomForest=r2_score(y_test , y_pred_RandomForest)
    compering_table.append(["RandomForest",r2_RandomForest])
    
    sns.regplot(y_test, y_pred_RandomForest,scatter_kws = {'color': 'purple'}, line_kws = {'color': 'red'},ax=ax3).set_title(' best n = '+str(df.iloc[4]['best_params']['n_estimators'])+' depth= '+str(df.iloc[4]['best_params']['max_depth'])+" r2 ="+str(round(r2_RandomForest,2)))
    
    fig.suptitle('compering different parameters in random forest regression')
    
    plt.show()
    
    plt.clf()
    
    #################################### knn ###################################
    fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
    
    # knn with 3 neighbors
    KNeighbors=KNeighborsClassifier(3)
    
    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(KNeighbors,X,y,dataset)
    
    KNeighbors.fit(X_train,y_train)
    y_pred_KNeighbors = KNeighbors.predict(X_test)
    r2_KNeighbors=r2_score(y_test , y_pred_KNeighbors)

    sns.regplot(y_test, y_pred_KNeighbors,scatter_kws = {'color': 'magenta'}, line_kws = {'color': 'red'},ax=ax1).set_title('minimum k =3 :r2= '+str(round( r2_KNeighbors,2)))
    
    # knn with 20 neighbors
    
    KNeighbors=KNeighborsClassifier(20)
    
    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(KNeighbors,X,y,dataset)
    
    KNeighbors.fit(X_train,y_train)
    y_pred_KNeighbors = KNeighbors.predict(X_test)
    r2_KNeighbors=r2_score(y_test , y_pred_KNeighbors)

    sns.regplot(y_test, y_pred_KNeighbors,scatter_kws = {'color': 'magenta'}, line_kws = {'color': 'red'},ax=ax2).set_title(' maximum k =20 :r2= '+str(round( r2_KNeighbors,2)))

    # knn with 3the best parametes
    
    KNeighbors=KNeighborsClassifier(n_neighbors=df.iloc[5]['best_params']['n_neighbors'], metric = 'minkowski',p=2)
    
    #backword elimination
    X_train, X_test, y_train, y_test=backwordelem(KNeighbors,X,y,dataset)
    
    KNeighbors.fit(X_train,y_train)
    y_pred_KNeighbors = KNeighbors.predict(X_test)
    r2_KNeighbors=r2_score(y_test , y_pred_KNeighbors)
    compering_table.append(["KNeighbors",r2_KNeighbors])
    
    sns.regplot(y_test, y_pred_KNeighbors,scatter_kws = {'color': 'magenta'}, line_kws = {'color': 'red'},ax=ax3).set_title('best k = '+str(df.iloc[5]['best_params']['n_neighbors'])+' r2 = '+str(round( r2_KNeighbors,2)))
    
    fig.suptitle('compering different parameters in KNN regression')
    
    plt.show()

    plt.clf()

        
    comperison=pd.DataFrame(compering_table,columns=("regression","r squerd"))
    print(comperison)
    print(df)
    plt.clf()
    
    dataset = pd.read_csv('COVID-19.csv')
    X = dataset.iloc[:,5:22].values

    
